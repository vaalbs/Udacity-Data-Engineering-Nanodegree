# Data Engineering Projects

## Project 1: Data Modeling with Postgres

In this project, I applied what I've learned on data modeling with PostgreSQL and build an ETL pipeline using Python. To complete the project, I defined fact and dimension tables for a star schema for a particular analytic focus, and writed an ETL pipeline that transfers data from files in two local directories into these tables in PostgreSQL using Python and SQL.

Link: [Data_Modeling_with_Postgres](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/data-modeling/data-modeling-with-postgrees)

## Project 2: Data Modeling with Cassandra

In this project, I would be applying Data Modeling with Apache Cassandra and complete an ETL pipeline using Python. I am provided with part of the ETL pipeline that transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables.

Link : [Data_Modeling_with_Apache_Cassandra](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/data-modeling/data-modeling-with-cassandra)

## Project 3: Data Warehouse

In this project, I applied what I've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, I loaded data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

Link - [Data_Warehouse](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/cloud-data-warehouses)

## Project 4: Data Lake

In this project, I applied what I've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, I needed to load data from S3, process the data into analytics tables using Spark, and load them back into S3. I deployed this Spark process on a cluster using AWS.

Link: [Data_Lake](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/data-lake)
