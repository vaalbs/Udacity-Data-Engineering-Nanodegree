# Data Engineering Projects

## Project 1: [Data Modeling with Postgres](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/data-modeling/data-modeling-with-postgrees)

In this project, I applied what I've learned on data modeling with PostgreSQL and build an ETL pipeline using Python. To complete the project, I defined fact and dimension tables for a star schema for a particular analytic focus, and writed an ETL pipeline that transfers data from files in two local directories into these tables in PostgreSQL using Python and SQL.

## Project 2: [Data Modeling with Apache Cassandra](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/data-modeling/data-modeling-with-cassandra)

In this project, I would be applying Data Modeling with Apache Cassandra and complete an ETL pipeline using Python. I am provided with part of the ETL pipeline that transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables.

## Project 3: [Data Warehouse](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/cloud-data-warehouses)

In this project, I applied what I've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, I loaded data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## Project 4: [Data Lake](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/data-lake)

In this project, I applied what I've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, I needed to load data from S3, process the data into analytics tables using Spark, and load them back into S3. I deployed this Spark process on a cluster using AWS.

## Project 5: [Data Pipelines with Airflow](https://github.com/vaalbs/Udacity-Data-Engineering-Nanodegree/tree/main/data-pipelines)

In this project, I have to create our own custom operators to perform tasks such as staging the data, filling the data warehouse and running checks on the data as the final step. I have been provided with four empty operators that need to be implemented into functional pieces of a data pipeline.
